import os
import os.path as osp
import shutil
import dateutil
import argparse
from datetime import datetime as dt

from dotenv import load_dotenv

import pytorch_lightning as pl
from base_pl_model import BasePLModel
from config import (
    cfg_to_parser_args,
    cfg,
    cfg_from_file,
    rsetattr,
    flatten_json_iterative_solution,
)


def parse_args_and_set_config():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    cfg_parser_args = cfg_to_parser_args()
    for arg_name, arg_kwargs in cfg_parser_args.items():
        if "_" in arg_name:
            parser.add_argument(
                f"--{arg_name}", f"--{arg_name.replace('_', '-')}", **arg_kwargs,
            )
        else:
            parser.add_argument(f"--{arg_name}", **arg_kwargs)
    trainer_default_params = pl.Trainer.default_attributes()
    for attr in ["gpus", "max_epochs", "max_nb_epochs"]:
        del trainer_default_params[attr]
    for arg in trainer_default_params:
        parser.add_argument(
            f"--{arg}",
            f"--{arg.replace('_', '-')}",
            default=trainer_default_params[arg],
            dest=arg,
            help="autogenerated by pl.Trainer",
        )

    args = parser.parse_args()
    cfg_from_file(args.cfg_file)
    for argname in cfg_parser_args.keys():
        arg_to_cfg(args, argname, cfg, argname)

    return args


def arg_to_cfg(arg, arg_name, cfg, cfg_attr_target):
    arg_value = getattr(arg, arg_name, None)
    if arg_value is not None:
        rsetattr(cfg, cfg_attr_target, arg_value)


if __name__ == "__main__":
    args = parse_args_and_set_config()
    # Overwrite

    model = BasePLModel(cfg)
    trainer = pl.Trainer(
        # gpus=cfg.gpus, # already in args
        max_epochs=cfg.train.epochs,
        **args.__dict__,
    )
    if args.eval:
        pass
    elif args.test:
        pass
    else:
        model.init_log()
